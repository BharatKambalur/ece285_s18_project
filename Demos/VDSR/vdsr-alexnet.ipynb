{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import alexnet as dn\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "# Imports\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# used for logging to TensorBoard\n",
    "# from tensorboard_logger import configure, log_value\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch DenseNet Training')\n",
    "parser.add_argument('--epochs', default=300, type=int,\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int,\n",
    "                    help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    help='print frequency (default: 10)')\n",
    "parser.add_argument('--layers', default=100, type=int,\n",
    "                    help='total number of layers (default: 100)')\n",
    "parser.add_argument('--growth', default=12, type=int,\n",
    "                    help='number of new channels per layer (default: 12)')\n",
    "parser.add_argument('--droprate', default=0, type=float,\n",
    "                    help='dropout probability (default: 0.0)')\n",
    "parser.add_argument('--no-augment', dest='augment', action='store_false',\n",
    "                    help='whether to use standard augmentation (default: True)')\n",
    "parser.add_argument('--reduce', default=0.5, type=float,\n",
    "                    help='compression rate in transition stage (default: 0.5)')\n",
    "parser.add_argument('--no-bottleneck', dest='bottleneck', action='store_false',\n",
    "                    help='To not use bottleneck block')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--name', default='DenseNet_BC_100_12', type=str,\n",
    "                    help='name of experiment')\n",
    "# parser.add_argument('--tensorboard',\n",
    "#                     help='Log progress to TensorBoard', action='store_true')\n",
    "parser.set_defaults(bottleneck=True)\n",
    "parser.set_defaults(augment=True)\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "# dbpn_args = ['feat0.conv.weight', 'feat0.conv.bias', 'feat0.act.weight', 'feat1.conv.weight', 'feat1.conv.bias', 'feat1.act.weight', 'up1.up_conv1.deconv.weight', 'up1.up_conv1.deconv.bias', 'up1.up_conv1.act.weight', 'up1.up_conv2.conv.weight', 'up1.up_conv2.conv.bias', 'up1.up_conv2.act.weight', 'up1.up_conv3.deconv.weight', 'up1.up_conv3.deconv.bias', 'up1.up_conv3.act.weight', 'down1.down_conv1.conv.weight', 'down1.down_conv1.conv.bias', 'down1.down_conv1.act.weight', 'down1.down_conv2.deconv.weight', 'down1.down_conv2.deconv.bias', 'down1.down_conv2.act.weight', 'down1.down_conv3.conv.weight', 'down1.down_conv3.conv.bias', 'down1.down_conv3.act.weight', 'up2.up_conv1.deconv.weight', 'up2.up_conv1.deconv.bias', 'up2.up_conv1.act.weight', 'up2.up_conv2.conv.weight', 'up2.up_conv2.conv.bias', 'up2.up_conv2.act.weight', 'up2.up_conv3.deconv.weight', 'up2.up_conv3.deconv.bias', 'up2.up_conv3.act.weight', 'down2.conv.conv.weight', 'down2.conv.conv.bias', 'down2.conv.act.weight', 'down2.down_conv1.conv.weight', 'down2.down_conv1.conv.bias', 'down2.down_conv1.act.weight', 'down2.down_conv2.deconv.weight', 'down2.down_conv2.deconv.bias', 'down2.down_conv2.act.weight', 'down2.down_conv3.conv.weight', 'down2.down_conv3.conv.bias', 'down2.down_conv3.act.weight', 'up3.conv.conv.weight', 'up3.conv.conv.bias', 'up3.conv.act.weight', 'up3.up_conv1.deconv.weight', 'up3.up_conv1.deconv.bias', 'up3.up_conv1.act.weight', 'up3.up_conv2.conv.weight', 'up3.up_conv2.conv.bias', 'up3.up_conv2.act.weight', 'up3.up_conv3.deconv.weight', 'up3.up_conv3.deconv.bias', 'up3.up_conv3.act.weight', 'down3.conv.conv.weight', 'down3.conv.conv.bias', 'down3.conv.act.weight', 'down3.down_conv1.conv.weight', 'down3.down_conv1.conv.bias', 'down3.down_conv1.act.weight', 'down3.down_conv2.deconv.weight', 'down3.down_conv2.deconv.bias', 'down3.down_conv2.act.weight', 'down3.down_conv3.conv.weight', 'down3.down_conv3.conv.bias', 'down3.down_conv3.act.weight', 'up4.conv.conv.weight', 'up4.conv.conv.bias', 'up4.conv.act.weight', 'up4.up_conv1.deconv.weight', 'up4.up_conv1.deconv.bias', 'up4.up_conv1.act.weight', 'up4.up_conv2.conv.weight', 'up4.up_conv2.conv.bias', 'up4.up_conv2.act.weight', 'up4.up_conv3.deconv.weight', 'up4.up_conv3.deconv.bias', 'up4.up_conv3.act.weight', 'down4.conv.conv.weight', 'down4.conv.conv.bias', 'down4.conv.act.weight', 'down4.down_conv1.conv.weight', 'down4.down_conv1.conv.bias', 'down4.down_conv1.act.weight', 'down4.down_conv2.deconv.weight', 'down4.down_conv2.deconv.bias', 'down4.down_conv2.act.weight', 'down4.down_conv3.conv.weight', 'down4.down_conv3.conv.bias', 'down4.down_conv3.act.weight', 'up5.conv.conv.weight', 'up5.conv.conv.bias', 'up5.conv.act.weight', 'up5.up_conv1.deconv.weight', 'up5.up_conv1.deconv.bias', 'up5.up_conv1.act.weight', 'up5.up_conv2.conv.weight', 'up5.up_conv2.conv.bias', 'up5.up_conv2.act.weight', 'up5.up_conv3.deconv.weight', 'up5.up_conv3.deconv.bias', 'up5.up_conv3.act.weight', 'down5.conv.conv.weight', 'down5.conv.conv.bias', 'down5.conv.act.weight', 'down5.down_conv1.conv.weight', 'down5.down_conv1.conv.bias', 'down5.down_conv1.act.weight', 'down5.down_conv2.deconv.weight', 'down5.down_conv2.deconv.bias', 'down5.down_conv2.act.weight', 'down5.down_conv3.conv.weight', 'down5.down_conv3.conv.bias', 'down5.down_conv3.act.weight', 'up6.conv.conv.weight', 'up6.conv.conv.bias', 'up6.conv.act.weight', 'up6.up_conv1.deconv.weight', 'up6.up_conv1.deconv.bias', 'up6.up_conv1.act.weight', 'up6.up_conv2.conv.weight', 'up6.up_conv2.conv.bias', 'up6.up_conv2.act.weight', 'up6.up_conv3.deconv.weight', 'up6.up_conv3.deconv.bias', 'up6.up_conv3.act.weight', 'down6.conv.conv.weight', 'down6.conv.conv.bias', 'down6.conv.act.weight', 'down6.down_conv1.conv.weight', 'down6.down_conv1.conv.bias', 'down6.down_conv1.act.weight', 'down6.down_conv2.deconv.weight', 'down6.down_conv2.deconv.bias', 'down6.down_conv2.act.weight', 'down6.down_conv3.conv.weight', 'down6.down_conv3.conv.bias', 'down6.down_conv3.act.weight', 'up7.conv.conv.weight', 'up7.conv.conv.bias', 'up7.conv.act.weight', 'up7.up_conv1.deconv.weight', 'up7.up_conv1.deconv.bias', 'up7.up_conv1.act.weight', 'up7.up_conv2.conv.weight', 'up7.up_conv2.conv.bias', 'up7.up_conv2.act.weight', 'up7.up_conv3.deconv.weight', 'up7.up_conv3.deconv.bias', 'up7.up_conv3.act.weight', 'output_conv.conv.weight', 'output_conv.conv.bias', 'conv1.weight']\n",
    "# densenet_args = ['block1.layer.0.bn1.weight', 'block1.layer.0.bn1.bias', 'block1.layer.0.bn1.running_mean', 'block1.layer.0.bn1.running_var', 'block1.layer.0.conv1.weight', 'block1.layer.1.bn1.weight', 'block1.layer.1.bn1.bias', 'block1.layer.1.bn1.running_mean', 'block1.layer.1.bn1.running_var', 'block1.layer.1.conv1.weight', 'block1.layer.2.bn1.weight', 'block1.layer.2.bn1.bias', 'block1.layer.2.bn1.running_mean', 'block1.layer.2.bn1.running_var', 'block1.layer.2.conv1.weight', 'block1.layer.3.bn1.weight', 'block1.layer.3.bn1.bias', 'block1.layer.3.bn1.running_mean', 'block1.layer.3.bn1.running_var', 'block1.layer.3.conv1.weight', 'block1.layer.4.bn1.weight', 'block1.layer.4.bn1.bias', 'block1.layer.4.bn1.running_mean', 'block1.layer.4.bn1.running_var', 'block1.layer.4.conv1.weight', 'block1.layer.5.bn1.weight', 'block1.layer.5.bn1.bias', 'block1.layer.5.bn1.running_mean', 'block1.layer.5.bn1.running_var', 'block1.layer.5.conv1.weight', 'block1.layer.6.bn1.weight', 'block1.layer.6.bn1.bias', 'block1.layer.6.bn1.running_mean', 'block1.layer.6.bn1.running_var', 'block1.layer.6.conv1.weight', 'block1.layer.7.bn1.weight', 'block1.layer.7.bn1.bias', 'block1.layer.7.bn1.running_mean', 'block1.layer.7.bn1.running_var', 'block1.layer.7.conv1.weight', 'block1.layer.8.bn1.weight', 'block1.layer.8.bn1.bias', 'block1.layer.8.bn1.running_mean', 'block1.layer.8.bn1.running_var', 'block1.layer.8.conv1.weight', 'block1.layer.9.bn1.weight', 'block1.layer.9.bn1.bias', 'block1.layer.9.bn1.running_mean', 'block1.layer.9.bn1.running_var', 'block1.layer.9.conv1.weight', 'block1.layer.10.bn1.weight', 'block1.layer.10.bn1.bias', 'block1.layer.10.bn1.running_mean', 'block1.layer.10.bn1.running_var', 'block1.layer.10.conv1.weight', 'block1.layer.11.bn1.weight', 'block1.layer.11.bn1.bias', 'block1.layer.11.bn1.running_mean', 'block1.layer.11.bn1.running_var', 'block1.layer.11.conv1.weight', 'trans1.bn1.weight', 'trans1.bn1.bias', 'trans1.bn1.running_mean', 'trans1.bn1.running_var', 'trans1.conv1.weight', 'block2.layer.0.bn1.weight', 'block2.layer.0.bn1.bias', 'block2.layer.0.bn1.running_mean', 'block2.layer.0.bn1.running_var', 'block2.layer.0.conv1.weight', 'block2.layer.1.bn1.weight', 'block2.layer.1.bn1.bias', 'block2.layer.1.bn1.running_mean', 'block2.layer.1.bn1.running_var', 'block2.layer.1.conv1.weight', 'block2.layer.2.bn1.weight', 'block2.layer.2.bn1.bias', 'block2.layer.2.bn1.running_mean', 'block2.layer.2.bn1.running_var', 'block2.layer.2.conv1.weight', 'block2.layer.3.bn1.weight', 'block2.layer.3.bn1.bias', 'block2.layer.3.bn1.running_mean', 'block2.layer.3.bn1.running_var', 'block2.layer.3.conv1.weight', 'block2.layer.4.bn1.weight', 'block2.layer.4.bn1.bias', 'block2.layer.4.bn1.running_mean', 'block2.layer.4.bn1.running_var', 'block2.layer.4.conv1.weight', 'block2.layer.5.bn1.weight', 'block2.layer.5.bn1.bias', 'block2.layer.5.bn1.running_mean', 'block2.layer.5.bn1.running_var', 'block2.layer.5.conv1.weight', 'block2.layer.6.bn1.weight', 'block2.layer.6.bn1.bias', 'block2.layer.6.bn1.running_mean', 'block2.layer.6.bn1.running_var', 'block2.layer.6.conv1.weight', 'block2.layer.7.bn1.weight', 'block2.layer.7.bn1.bias', 'block2.layer.7.bn1.running_mean', 'block2.layer.7.bn1.running_var', 'block2.layer.7.conv1.weight', 'block2.layer.8.bn1.weight', 'block2.layer.8.bn1.bias', 'block2.layer.8.bn1.running_mean', 'block2.layer.8.bn1.running_var', 'block2.layer.8.conv1.weight', 'block2.layer.9.bn1.weight', 'block2.layer.9.bn1.bias', 'block2.layer.9.bn1.running_mean', 'block2.layer.9.bn1.running_var', 'block2.layer.9.conv1.weight', 'block2.layer.10.bn1.weight', 'block2.layer.10.bn1.bias', 'block2.layer.10.bn1.running_mean', 'block2.layer.10.bn1.running_var', 'block2.layer.10.conv1.weight', 'block2.layer.11.bn1.weight', 'block2.layer.11.bn1.bias', 'block2.layer.11.bn1.running_mean', 'block2.layer.11.bn1.running_var', 'block2.layer.11.conv1.weight', 'trans2.bn1.weight', 'trans2.bn1.bias', 'trans2.bn1.running_mean', 'trans2.bn1.running_var', 'trans2.conv1.weight', 'block3.layer.0.bn1.weight', 'block3.layer.0.bn1.bias', 'block3.layer.0.bn1.running_mean', 'block3.layer.0.bn1.running_var', 'block3.layer.0.conv1.weight', 'block3.layer.1.bn1.weight', 'block3.layer.1.bn1.bias', 'block3.layer.1.bn1.running_mean', 'block3.layer.1.bn1.running_var', 'block3.layer.1.conv1.weight', 'block3.layer.2.bn1.weight', 'block3.layer.2.bn1.bias', 'block3.layer.2.bn1.running_mean', 'block3.layer.2.bn1.running_var', 'block3.layer.2.conv1.weight', 'block3.layer.3.bn1.weight', 'block3.layer.3.bn1.bias', 'block3.layer.3.bn1.running_mean', 'block3.layer.3.bn1.running_var', 'block3.layer.3.conv1.weight', 'block3.layer.4.bn1.weight', 'block3.layer.4.bn1.bias', 'block3.layer.4.bn1.running_mean', 'block3.layer.4.bn1.running_var', 'block3.layer.4.conv1.weight', 'block3.layer.5.bn1.weight', 'block3.layer.5.bn1.bias', 'block3.layer.5.bn1.running_mean', 'block3.layer.5.bn1.running_var', 'block3.layer.5.conv1.weight', 'block3.layer.6.bn1.weight', 'block3.layer.6.bn1.bias', 'block3.layer.6.bn1.running_mean', 'block3.layer.6.bn1.running_var', 'block3.layer.6.conv1.weight', 'block3.layer.7.bn1.weight', 'block3.layer.7.bn1.bias', 'block3.layer.7.bn1.running_mean', 'block3.layer.7.bn1.running_var', 'block3.layer.7.conv1.weight', 'block3.layer.8.bn1.weight', 'block3.layer.8.bn1.bias', 'block3.layer.8.bn1.running_mean', 'block3.layer.8.bn1.running_var', 'block3.layer.8.conv1.weight', 'block3.layer.9.bn1.weight', 'block3.layer.9.bn1.bias', 'block3.layer.9.bn1.running_mean', 'block3.layer.9.bn1.running_var', 'block3.layer.9.conv1.weight', 'block3.layer.10.bn1.weight', 'block3.layer.10.bn1.bias', 'block3.layer.10.bn1.running_mean', 'block3.layer.10.bn1.running_var', 'block3.layer.10.conv1.weight', 'block3.layer.11.bn1.weight', 'block3.layer.11.bn1.bias', 'block3.layer.11.bn1.running_mean', 'block3.layer.11.bn1.running_var', 'block3.layer.11.conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'fc.weight', 'fc.bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arg_string, train_file, test_file):\n",
    "    global args, best_prec1\n",
    "    #setting up integrating the two models\n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "    upscale_factor = 2 # Can be 2, 4 or 8\n",
    "    cuda = True # Set True if you're using GPU\n",
    "    gpus=2\n",
    "\n",
    "    seed = 123\n",
    "    \n",
    "    arg_list = arg_string.split()\n",
    "    args = parser.parse_args(arg_list) #Adjusted for arguements\n",
    "#     if args.tensorboard: configure(\"runs/%s\"%(args.name))\n",
    "    \n",
    "    # Data loading code\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    \n",
    "    if args.augment:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            \n",
    "#             transforms.Resize((128,128),Image.BICUBIC), #x4 scaling\n",
    "            transforms.Resize((32,32),Image.BICUBIC), #x4 scaling\n",
    "#             transforms.Transpose([1, 2, 0])\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    transform_test = transforms.Compose([\n",
    "#         transforms.Resize((128,128),Image.BICUBIC), #x4 scaling\n",
    "        transforms.Resize((32,32),Image.BICUBIC), #x4 scaling\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(os.path.join('datasets','ee285s-public','cifar100'), train=True, download=True,\n",
    "                         transform=transform_train),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(os.path.join('datasets','ee285s-public','cifar100'), train=False, transform=transform_test),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # create model\n",
    "    \n",
    "    model = dn.AlexNet(100)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    \n",
    "    pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "    pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "    # Load the pretrained model\n",
    "#     vdsr_model = torch.load(\"model/model_epoch_50.pth\")[\"model\"]\n",
    "    vdsr_model = torch.load(\"model/model_epoch_50.pth\")[\"model\"]\n",
    "    vdsr_model = vdsr_model.cuda()\n",
    "    \n",
    "#     for param in vdsr_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "    \n",
    "    print('Number of sr model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in vdsr_model.parameters()])))\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "        \n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                nesterov=True,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(param_list, args.lr,\n",
    "#                                 momentum=args.momentum,\n",
    "#                                 nesterov=True,\n",
    "#                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        prec2 = train(train_loader, model, vdsr_model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, vdsr_model, criterion, epoch)\n",
    "        \n",
    "        if (epoch == 0):\n",
    "            np.save(current_dir + train_file,prec2)\n",
    "            np.save(current_dir + test_file,prec1)\n",
    "        else:\n",
    "            train_acc = np.load(current_dir + train_file)\n",
    "            test_acc = np.load(current_dir + test_file)\n",
    "            t_val = prec2\n",
    "            v_val = prec1\n",
    "            np.save(current_dir + train_file,np.array(np.append(train_acc,t_val)))\n",
    "            np.save(current_dir + test_file,np.array(np.append(test_acc,v_val)))\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "#         print(model.state_dict())\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "    print('Best accuracy: ', best_prec1)\n",
    "\n",
    "def colorize(y, ycbcr): \n",
    "    img = np.zeros((y.shape[0], y.shape[1], 3), np.uint8)\n",
    "    img[:,:,0] = y\n",
    "    img[:,:,1] = ycbcr[:,:,1]\n",
    "    img[:,:,2] = ycbcr[:,:,2]\n",
    "    img = Image.fromarray(img, \"YCbCr\").convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def rgb_to_ycbcr(input):\n",
    "    # input is mini-batch N x 3 x H x W of an RGB image\n",
    "    output = Variable(input.data.new(*input.size()))\n",
    "    output[:, 0, :, :] = input[:, 0, :, :] * 65.481 + input[:, 1, :, :] * 128.553 + input[:, 2, :, :] * 24.966 + 16\n",
    "    output[:, 1, :, :] = -input[:, 0, :, :] * 37.393 - input[:, 1, :, :] * 74.203 + input[:, 2, :, :] * 112. + 128\n",
    "    output[:, 2, :, :] = input[:, 0, :, :] * 112.0 - input[:, 1, :, :] * 93.786 - input[:, 2, :, :] * 18.214 + 128\n",
    "    return output\n",
    "\n",
    "def train(train_loader, model, sr_model, criterion, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        print(input_var.shape)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "# im_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\n",
    "        # compute output\n",
    "        input_y = rgb_to_ycbcr(input_var)\n",
    "#         print(input_y.shape[0],input_y.shape[1])\n",
    "        input_y = input_y[:,0,:,:].view(1,-1,input_y.shape[2],input_y.shape[3])\n",
    "#         input_y = Variable(input_y.view(1, -1, input_y.shape[2], input_y.shape[3]))\n",
    "        print(input_y.shape)\n",
    "#         sr_out = sr_model(input_y[:,0,:,:].view(-1,1,128,128))\n",
    "        sr_out = sr_model(input_y)\n",
    "        sr_out = sr_out.cpu()\n",
    "        im_h_y = sr_out.data[0].numpy().astype(np.float32)\n",
    "        im_h_y = im_h_y * 255.\n",
    "        im_h_y[im_h_y < 0] = 0\n",
    "        im_h_y[im_h_y > 255.] = 255.\n",
    "        im_h_y = im_h_y[0,:,:]\n",
    "        im_h = np.array(colorize(im_h_y,input_y))\n",
    "        print(im_h.shape)\n",
    "#         print(im_h.size)\n",
    "        im_h = Variable(torch.from_numpy(im_h).float()).view(1, -1, im_h.shape[0], im_h.shape[1])\n",
    "#         im_h = transforms.ToTensor()(im_h).unsqueeze(0).cuda()\n",
    "#         print(im_h.shape)\n",
    "        output = model(im_h.cuda())\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "        return top1.avg\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('train_loss', losses.avg, epoch)\n",
    "#         log_value('train_acc', top1.avg, epoch)\n",
    "\n",
    "def validate(val_loader, model, sr_model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        input_y = rgb_to_ycbcr(input_var)\n",
    "        input_y = input_y[:,0,:,:].view(1,-1,input_y.shape[2],input_y.shape[3])\n",
    "#         input_y = Variable(input_y.view(1, -1, input_y.shape[2], input_y.shape[3]))\n",
    "        print(input_y.shape)\n",
    "#         sr_out = sr_model(input_y[:,0,:,:].view(-1,1,128,128))\n",
    "        sr_out = sr_model(input_y)\n",
    "        sr_out = sr_out.cpu()\n",
    "        im_h_y = sr_out.data[0].numpy().astype(np.float32)\n",
    "        im_h_y = im_h_y * 255.\n",
    "        im_h_y[im_h_y < 0] = 0\n",
    "        im_h_y[im_h_y > 255.] = 255.\n",
    "        im_h_y = im_h_y[0,:,:]\n",
    "        im_h = np.array(colorize(im_h_y,input_y))\n",
    "        print(im_h.shape)\n",
    "#         print(im_h.size)\n",
    "        im_h = Variable(torch.from_numpy(im_h).float()).view(1, -1, im_h.shape[0], im_h.shape[1])\n",
    "\n",
    "        im_h = im_h.cuda()\n",
    "        output = model(im_h)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('val_loss', losses.avg, epoch)\n",
    "#         log_value('val_acc', top1.avg, epoch)\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"runs/%s/\"%(args.name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'runs/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 150)) * (0.1 ** (epoch // 225))\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('learning_rate', lr, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Number of model parameters: 2495396\n",
      "Number of sr model parameters: 664704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 64, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[64, 1, 3, 3], so expected input[1, 64, 32, 32] to have 1 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-62193673b5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0marg_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"--batch-size 64 --name VDSR-AlexNet-x4 --resume runs/VDSR-AlexNet-x4/checkpoint.pth.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f4fe61de918f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(arg_string, train_file, test_file)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mprec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvdsr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f4fe61de918f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, sr_model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m#         sr_out = sr_model(input_y[:,0,:,:].view(-1,1,128,128))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0msr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0msr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mim_h_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece285/pytorch-vdsr/vdsr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight[64, 1, 3, 3], so expected input[1, 64, 32, 32] to have 1 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "train_file = '/train_vdsr_x4.npy'\n",
    "test_file = '/test_vdsr_x4.npy'\n",
    "new = True\n",
    "if new:\n",
    "    arg_string = \"--batch-size 1 --name VDSR-AlexNet-x4\"\n",
    "else:\n",
    "    arg_string = \"--batch-size 1 --name VDSR-AlexNet-x4 --resume runs/VDSR-AlexNet-x4/checkpoint.pth.tar\"\n",
    "train_acc, test_acc = main(arg_string, train_file, test_file)\n",
    "plt.figure()\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
