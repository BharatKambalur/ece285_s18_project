{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import dbpn_densenet as dn\n",
    "\n",
    "# Imports\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# used for logging to TensorBoard\n",
    "# from tensorboard_logger import configure, log_value\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch DenseNet Training')\n",
    "parser.add_argument('--epochs', default=300, type=int,\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int,\n",
    "                    help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    help='print frequency (default: 10)')\n",
    "parser.add_argument('--layers', default=100, type=int,\n",
    "                    help='total number of layers (default: 100)')\n",
    "parser.add_argument('--growth', default=12, type=int,\n",
    "                    help='number of new channels per layer (default: 12)')\n",
    "parser.add_argument('--droprate', default=0, type=float,\n",
    "                    help='dropout probability (default: 0.0)')\n",
    "parser.add_argument('--no-augment', dest='augment', action='store_false',\n",
    "                    help='whether to use standard augmentation (default: True)')\n",
    "parser.add_argument('--reduce', default=0.5, type=float,\n",
    "                    help='compression rate in transition stage (default: 0.5)')\n",
    "parser.add_argument('--no-bottleneck', dest='bottleneck', action='store_false',\n",
    "                    help='To not use bottleneck block')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--name', default='DenseNet_BC_100_12', type=str,\n",
    "                    help='name of experiment')\n",
    "# parser.add_argument('--tensorboard',\n",
    "#                     help='Log progress to TensorBoard', action='store_true')\n",
    "parser.set_defaults(bottleneck=True)\n",
    "parser.set_defaults(augment=True)\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "dbpn_args = ['feat0.conv.weight', 'feat0.conv.bias', 'feat0.act.weight', 'feat1.conv.weight', 'feat1.conv.bias', 'feat1.act.weight', 'up1.up_conv1.deconv.weight', 'up1.up_conv1.deconv.bias', 'up1.up_conv1.act.weight', 'up1.up_conv2.conv.weight', 'up1.up_conv2.conv.bias', 'up1.up_conv2.act.weight', 'up1.up_conv3.deconv.weight', 'up1.up_conv3.deconv.bias', 'up1.up_conv3.act.weight', 'down1.down_conv1.conv.weight', 'down1.down_conv1.conv.bias', 'down1.down_conv1.act.weight', 'down1.down_conv2.deconv.weight', 'down1.down_conv2.deconv.bias', 'down1.down_conv2.act.weight', 'down1.down_conv3.conv.weight', 'down1.down_conv3.conv.bias', 'down1.down_conv3.act.weight', 'up2.up_conv1.deconv.weight', 'up2.up_conv1.deconv.bias', 'up2.up_conv1.act.weight', 'up2.up_conv2.conv.weight', 'up2.up_conv2.conv.bias', 'up2.up_conv2.act.weight', 'up2.up_conv3.deconv.weight', 'up2.up_conv3.deconv.bias', 'up2.up_conv3.act.weight', 'down2.conv.conv.weight', 'down2.conv.conv.bias', 'down2.conv.act.weight', 'down2.down_conv1.conv.weight', 'down2.down_conv1.conv.bias', 'down2.down_conv1.act.weight', 'down2.down_conv2.deconv.weight', 'down2.down_conv2.deconv.bias', 'down2.down_conv2.act.weight', 'down2.down_conv3.conv.weight', 'down2.down_conv3.conv.bias', 'down2.down_conv3.act.weight', 'up3.conv.conv.weight', 'up3.conv.conv.bias', 'up3.conv.act.weight', 'up3.up_conv1.deconv.weight', 'up3.up_conv1.deconv.bias', 'up3.up_conv1.act.weight', 'up3.up_conv2.conv.weight', 'up3.up_conv2.conv.bias', 'up3.up_conv2.act.weight', 'up3.up_conv3.deconv.weight', 'up3.up_conv3.deconv.bias', 'up3.up_conv3.act.weight', 'down3.conv.conv.weight', 'down3.conv.conv.bias', 'down3.conv.act.weight', 'down3.down_conv1.conv.weight', 'down3.down_conv1.conv.bias', 'down3.down_conv1.act.weight', 'down3.down_conv2.deconv.weight', 'down3.down_conv2.deconv.bias', 'down3.down_conv2.act.weight', 'down3.down_conv3.conv.weight', 'down3.down_conv3.conv.bias', 'down3.down_conv3.act.weight', 'up4.conv.conv.weight', 'up4.conv.conv.bias', 'up4.conv.act.weight', 'up4.up_conv1.deconv.weight', 'up4.up_conv1.deconv.bias', 'up4.up_conv1.act.weight', 'up4.up_conv2.conv.weight', 'up4.up_conv2.conv.bias', 'up4.up_conv2.act.weight', 'up4.up_conv3.deconv.weight', 'up4.up_conv3.deconv.bias', 'up4.up_conv3.act.weight', 'down4.conv.conv.weight', 'down4.conv.conv.bias', 'down4.conv.act.weight', 'down4.down_conv1.conv.weight', 'down4.down_conv1.conv.bias', 'down4.down_conv1.act.weight', 'down4.down_conv2.deconv.weight', 'down4.down_conv2.deconv.bias', 'down4.down_conv2.act.weight', 'down4.down_conv3.conv.weight', 'down4.down_conv3.conv.bias', 'down4.down_conv3.act.weight', 'up5.conv.conv.weight', 'up5.conv.conv.bias', 'up5.conv.act.weight', 'up5.up_conv1.deconv.weight', 'up5.up_conv1.deconv.bias', 'up5.up_conv1.act.weight', 'up5.up_conv2.conv.weight', 'up5.up_conv2.conv.bias', 'up5.up_conv2.act.weight', 'up5.up_conv3.deconv.weight', 'up5.up_conv3.deconv.bias', 'up5.up_conv3.act.weight', 'down5.conv.conv.weight', 'down5.conv.conv.bias', 'down5.conv.act.weight', 'down5.down_conv1.conv.weight', 'down5.down_conv1.conv.bias', 'down5.down_conv1.act.weight', 'down5.down_conv2.deconv.weight', 'down5.down_conv2.deconv.bias', 'down5.down_conv2.act.weight', 'down5.down_conv3.conv.weight', 'down5.down_conv3.conv.bias', 'down5.down_conv3.act.weight', 'up6.conv.conv.weight', 'up6.conv.conv.bias', 'up6.conv.act.weight', 'up6.up_conv1.deconv.weight', 'up6.up_conv1.deconv.bias', 'up6.up_conv1.act.weight', 'up6.up_conv2.conv.weight', 'up6.up_conv2.conv.bias', 'up6.up_conv2.act.weight', 'up6.up_conv3.deconv.weight', 'up6.up_conv3.deconv.bias', 'up6.up_conv3.act.weight', 'down6.conv.conv.weight', 'down6.conv.conv.bias', 'down6.conv.act.weight', 'down6.down_conv1.conv.weight', 'down6.down_conv1.conv.bias', 'down6.down_conv1.act.weight', 'down6.down_conv2.deconv.weight', 'down6.down_conv2.deconv.bias', 'down6.down_conv2.act.weight', 'down6.down_conv3.conv.weight', 'down6.down_conv3.conv.bias', 'down6.down_conv3.act.weight', 'up7.conv.conv.weight', 'up7.conv.conv.bias', 'up7.conv.act.weight', 'up7.up_conv1.deconv.weight', 'up7.up_conv1.deconv.bias', 'up7.up_conv1.act.weight', 'up7.up_conv2.conv.weight', 'up7.up_conv2.conv.bias', 'up7.up_conv2.act.weight', 'up7.up_conv3.deconv.weight', 'up7.up_conv3.deconv.bias', 'up7.up_conv3.act.weight', 'output_conv.conv.weight', 'output_conv.conv.bias', 'conv1.weight']\n",
    "densenet_args = ['block1.layer.0.bn1.weight', 'block1.layer.0.bn1.bias', 'block1.layer.0.bn1.running_mean', 'block1.layer.0.bn1.running_var', 'block1.layer.0.conv1.weight', 'block1.layer.1.bn1.weight', 'block1.layer.1.bn1.bias', 'block1.layer.1.bn1.running_mean', 'block1.layer.1.bn1.running_var', 'block1.layer.1.conv1.weight', 'block1.layer.2.bn1.weight', 'block1.layer.2.bn1.bias', 'block1.layer.2.bn1.running_mean', 'block1.layer.2.bn1.running_var', 'block1.layer.2.conv1.weight', 'block1.layer.3.bn1.weight', 'block1.layer.3.bn1.bias', 'block1.layer.3.bn1.running_mean', 'block1.layer.3.bn1.running_var', 'block1.layer.3.conv1.weight', 'block1.layer.4.bn1.weight', 'block1.layer.4.bn1.bias', 'block1.layer.4.bn1.running_mean', 'block1.layer.4.bn1.running_var', 'block1.layer.4.conv1.weight', 'block1.layer.5.bn1.weight', 'block1.layer.5.bn1.bias', 'block1.layer.5.bn1.running_mean', 'block1.layer.5.bn1.running_var', 'block1.layer.5.conv1.weight', 'block1.layer.6.bn1.weight', 'block1.layer.6.bn1.bias', 'block1.layer.6.bn1.running_mean', 'block1.layer.6.bn1.running_var', 'block1.layer.6.conv1.weight', 'block1.layer.7.bn1.weight', 'block1.layer.7.bn1.bias', 'block1.layer.7.bn1.running_mean', 'block1.layer.7.bn1.running_var', 'block1.layer.7.conv1.weight', 'block1.layer.8.bn1.weight', 'block1.layer.8.bn1.bias', 'block1.layer.8.bn1.running_mean', 'block1.layer.8.bn1.running_var', 'block1.layer.8.conv1.weight', 'block1.layer.9.bn1.weight', 'block1.layer.9.bn1.bias', 'block1.layer.9.bn1.running_mean', 'block1.layer.9.bn1.running_var', 'block1.layer.9.conv1.weight', 'block1.layer.10.bn1.weight', 'block1.layer.10.bn1.bias', 'block1.layer.10.bn1.running_mean', 'block1.layer.10.bn1.running_var', 'block1.layer.10.conv1.weight', 'block1.layer.11.bn1.weight', 'block1.layer.11.bn1.bias', 'block1.layer.11.bn1.running_mean', 'block1.layer.11.bn1.running_var', 'block1.layer.11.conv1.weight', 'trans1.bn1.weight', 'trans1.bn1.bias', 'trans1.bn1.running_mean', 'trans1.bn1.running_var', 'trans1.conv1.weight', 'block2.layer.0.bn1.weight', 'block2.layer.0.bn1.bias', 'block2.layer.0.bn1.running_mean', 'block2.layer.0.bn1.running_var', 'block2.layer.0.conv1.weight', 'block2.layer.1.bn1.weight', 'block2.layer.1.bn1.bias', 'block2.layer.1.bn1.running_mean', 'block2.layer.1.bn1.running_var', 'block2.layer.1.conv1.weight', 'block2.layer.2.bn1.weight', 'block2.layer.2.bn1.bias', 'block2.layer.2.bn1.running_mean', 'block2.layer.2.bn1.running_var', 'block2.layer.2.conv1.weight', 'block2.layer.3.bn1.weight', 'block2.layer.3.bn1.bias', 'block2.layer.3.bn1.running_mean', 'block2.layer.3.bn1.running_var', 'block2.layer.3.conv1.weight', 'block2.layer.4.bn1.weight', 'block2.layer.4.bn1.bias', 'block2.layer.4.bn1.running_mean', 'block2.layer.4.bn1.running_var', 'block2.layer.4.conv1.weight', 'block2.layer.5.bn1.weight', 'block2.layer.5.bn1.bias', 'block2.layer.5.bn1.running_mean', 'block2.layer.5.bn1.running_var', 'block2.layer.5.conv1.weight', 'block2.layer.6.bn1.weight', 'block2.layer.6.bn1.bias', 'block2.layer.6.bn1.running_mean', 'block2.layer.6.bn1.running_var', 'block2.layer.6.conv1.weight', 'block2.layer.7.bn1.weight', 'block2.layer.7.bn1.bias', 'block2.layer.7.bn1.running_mean', 'block2.layer.7.bn1.running_var', 'block2.layer.7.conv1.weight', 'block2.layer.8.bn1.weight', 'block2.layer.8.bn1.bias', 'block2.layer.8.bn1.running_mean', 'block2.layer.8.bn1.running_var', 'block2.layer.8.conv1.weight', 'block2.layer.9.bn1.weight', 'block2.layer.9.bn1.bias', 'block2.layer.9.bn1.running_mean', 'block2.layer.9.bn1.running_var', 'block2.layer.9.conv1.weight', 'block2.layer.10.bn1.weight', 'block2.layer.10.bn1.bias', 'block2.layer.10.bn1.running_mean', 'block2.layer.10.bn1.running_var', 'block2.layer.10.conv1.weight', 'block2.layer.11.bn1.weight', 'block2.layer.11.bn1.bias', 'block2.layer.11.bn1.running_mean', 'block2.layer.11.bn1.running_var', 'block2.layer.11.conv1.weight', 'trans2.bn1.weight', 'trans2.bn1.bias', 'trans2.bn1.running_mean', 'trans2.bn1.running_var', 'trans2.conv1.weight', 'block3.layer.0.bn1.weight', 'block3.layer.0.bn1.bias', 'block3.layer.0.bn1.running_mean', 'block3.layer.0.bn1.running_var', 'block3.layer.0.conv1.weight', 'block3.layer.1.bn1.weight', 'block3.layer.1.bn1.bias', 'block3.layer.1.bn1.running_mean', 'block3.layer.1.bn1.running_var', 'block3.layer.1.conv1.weight', 'block3.layer.2.bn1.weight', 'block3.layer.2.bn1.bias', 'block3.layer.2.bn1.running_mean', 'block3.layer.2.bn1.running_var', 'block3.layer.2.conv1.weight', 'block3.layer.3.bn1.weight', 'block3.layer.3.bn1.bias', 'block3.layer.3.bn1.running_mean', 'block3.layer.3.bn1.running_var', 'block3.layer.3.conv1.weight', 'block3.layer.4.bn1.weight', 'block3.layer.4.bn1.bias', 'block3.layer.4.bn1.running_mean', 'block3.layer.4.bn1.running_var', 'block3.layer.4.conv1.weight', 'block3.layer.5.bn1.weight', 'block3.layer.5.bn1.bias', 'block3.layer.5.bn1.running_mean', 'block3.layer.5.bn1.running_var', 'block3.layer.5.conv1.weight', 'block3.layer.6.bn1.weight', 'block3.layer.6.bn1.bias', 'block3.layer.6.bn1.running_mean', 'block3.layer.6.bn1.running_var', 'block3.layer.6.conv1.weight', 'block3.layer.7.bn1.weight', 'block3.layer.7.bn1.bias', 'block3.layer.7.bn1.running_mean', 'block3.layer.7.bn1.running_var', 'block3.layer.7.conv1.weight', 'block3.layer.8.bn1.weight', 'block3.layer.8.bn1.bias', 'block3.layer.8.bn1.running_mean', 'block3.layer.8.bn1.running_var', 'block3.layer.8.conv1.weight', 'block3.layer.9.bn1.weight', 'block3.layer.9.bn1.bias', 'block3.layer.9.bn1.running_mean', 'block3.layer.9.bn1.running_var', 'block3.layer.9.conv1.weight', 'block3.layer.10.bn1.weight', 'block3.layer.10.bn1.bias', 'block3.layer.10.bn1.running_mean', 'block3.layer.10.bn1.running_var', 'block3.layer.10.conv1.weight', 'block3.layer.11.bn1.weight', 'block3.layer.11.bn1.bias', 'block3.layer.11.bn1.running_mean', 'block3.layer.11.bn1.running_var', 'block3.layer.11.conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'fc.weight', 'fc.bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, best_prec1\n",
    "    #setting up integrating the two models\n",
    "    current_dir = '/datasets/home/05/205/jmattern/285_proj/'\n",
    "    model_path = os.path.join(current_dir,'models/DBPN_x8.pth')\n",
    "    upscale_factor = 8 # Can be 2, 4 or 8\n",
    "    cuda = True # Set True if you're using GPU\n",
    "    gpus=2\n",
    "\n",
    "    seed = 123\n",
    "    \n",
    "    arg_list = \"--layers 40 --growth 12 --no-bottleneck --reduce 1.0 --name DenseNet-40-12\".split()\n",
    "    args = parser.parse_args(arg_list) #Adjusted for arguements\n",
    "#     if args.tensorboard: configure(\"runs/%s\"%(args.name))\n",
    "    \n",
    "    # Data loading code\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "    \n",
    "    if args.augment:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "        ])\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(os.path.join(current_dir,'data'), train=True, download=True,\n",
    "                         transform=transform_train),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(os.path.join(current_dir,'data'), train=False, transform=transform_test),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # create model\n",
    "    model = dn.DBPNDenseNet3(args.layers, 100, args.growth, reduction=args.reduce,\n",
    "                         bottleneck=args.bottleneck, dropRate=args.droprate, num_channels=3, base_filter=64,  feat = 256, num_stages=7, scale_factor=upscale_factor)\n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    #load the pretrained DBPN model\n",
    "    if os.path.exists(model_path):\n",
    "        dbpn_model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        own_state = model.state_dict()\n",
    "        \n",
    "        for name, param in dbpn_model.items():\n",
    "            name = name[7:]\n",
    "            if name not in own_state.keys():\n",
    "                 continue\n",
    "#             if isinstance(param, Parameter):\n",
    "#                 # backwards compatibility for serialized parameters\n",
    "#                 param = param.data\n",
    "            own_state[name].copy_(param)\n",
    "#         model.load_state_dict(dbpn_model)\n",
    "        print('Pre-trained SR model is loaded.')\n",
    "    else:\n",
    "        print('Directory path does not exist')\n",
    "        \n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "    \n",
    "    # for training on multiple GPUs. \n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    # model = model.cuda()\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    param_list = []\n",
    "    for name, param in model.named_parameters():\n",
    "#         p_dict = {}\n",
    "#         p_dict['params'] = param\n",
    "        if name in dbpn_model.keys():\n",
    "            param.requires_grad = False\n",
    "#             p_dict['lr'] = 0\n",
    "#         param_list.append(p_dict)\n",
    "        \n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                nesterov=True,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(param_list, args.lr,\n",
    "#                                 momentum=args.momentum,\n",
    "#                                 nesterov=True,\n",
    "#                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "#         print(model.state_dict())\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "    print('Best accuracy: ', best_prec1)\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('train_loss', losses.avg, epoch)\n",
    "#         log_value('train_acc', top1.avg, epoch)\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input = input.cuda()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('val_loss', losses.avg, epoch)\n",
    "#         log_value('val_acc', top1.avg, epoch)\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"runs/%s/\"%(args.name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'runs/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 150)) * (0.1 ** (epoch // 225))\n",
    "    # log to TensorBoard\n",
    "#     if args.tensorboard:\n",
    "#         log_value('learning_rate', lr, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/home/05/205/jmattern/285_proj/dbpn_densenet.py:116: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  torch.nn.init.kaiming_normal(m.weight)\n",
      "/datasets/home/05/205/jmattern/285_proj/dbpn_densenet.py:120: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  torch.nn.init.kaiming_normal(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBPNDenseNet3(\n",
      "  (feat0): ConvBlock(\n",
      "    (conv): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (act): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (feat1): ConvBlock(\n",
      "    (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (act): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (up1): UpBlock(\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down1): DownBlock(\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up2): UpBlock(\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down2): D_DownBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up3): D_UpBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down3): D_DownBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up4): D_UpBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down4): D_DownBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up5): D_UpBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down5): D_DownBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up6): D_UpBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (down6): D_DownBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv1): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv2): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down_conv3): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (up7): D_UpBlock(\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv1): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv2): ConvBlock(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (up_conv3): DeconvBlock(\n",
      "      (deconv): ConvTranspose2d(64, 64, kernel_size=(12, 12), stride=(8, 8), padding=(2, 2))\n",
      "      (act): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (output_conv): ConvBlock(\n",
      "    (conv): Conv2d(448, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (block1): DenseBlock(\n",
      "    (layer): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(36, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(60, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(72, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(84, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (6): BasicBlock(\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (7): BasicBlock(\n",
      "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(108, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (8): BasicBlock(\n",
      "        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(120, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (9): BasicBlock(\n",
      "        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(132, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (10): BasicBlock(\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(144, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (11): BasicBlock(\n",
      "        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(156, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (trans1): TransitionBlock(\n",
      "    (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv1): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block2): DenseBlock(\n",
      "    (layer): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(168, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(180, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(204, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(216, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(228, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (6): BasicBlock(\n",
      "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(240, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (7): BasicBlock(\n",
      "        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(252, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (8): BasicBlock(\n",
      "        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(264, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (9): BasicBlock(\n",
      "        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(276, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (10): BasicBlock(\n",
      "        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(288, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (11): BasicBlock(\n",
      "        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(300, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (trans2): TransitionBlock(\n",
      "    (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv1): Conv2d(312, 312, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block3): DenseBlock(\n",
      "    (layer): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(312, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(324, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(336, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (bn1): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(348, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(360, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (bn1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(372, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (6): BasicBlock(\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(384, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (7): BasicBlock(\n",
      "        (bn1): BatchNorm2d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(396, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (8): BasicBlock(\n",
      "        (bn1): BatchNorm2d(408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(408, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (9): BasicBlock(\n",
      "        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(420, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (10): BasicBlock(\n",
      "        (bn1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(432, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (11): BasicBlock(\n",
      "        (bn1): BatchNorm2d(444, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv1): Conv2d(444, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(456, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (fc): Linear(in_features=456, out_features=100, bias=True)\n",
      ")\n",
      "Pre-trained SR model is loaded.\n",
      "Number of model parameters: 24306306\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-21cfce3f66bf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-21cfce3f66bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/285_proj/dbpn_densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mconcat_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/285_proj/base_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/285_proj/base_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "model_path = '/datasets/home/05/205/jmattern/285_proj/models/DBPN_x8.pth'\n",
    "if os.path.exists(model_path):\n",
    "    print('good')\n",
    "else:\n",
    "    print('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/datasets/home/05/205/jmattern/285_proj', ['.ipynb_checkpoints', 'models', 'runs', '__pycache__'], ['sr_densenet.ipynb', 'LICENSE', 'train.py', 'dbpn.py', 'densenet.py', 'dbpn_densenet.py', 'base_networks.py', 'README.md']), ('/datasets/home/05/205/jmattern/285_proj/.ipynb_checkpoints', [], ['sr_densenet-checkpoint.ipynb']), ('/datasets/home/05/205/jmattern/285_proj/models', [], ['DBPN_x2.pth', 'DBPN_x8.pth']), ('/datasets/home/05/205/jmattern/285_proj/runs', [], []), ('/datasets/home/05/205/jmattern/285_proj/__pycache__', [], ['base_networks.cpython-36.pyc', 'dbpn_densenet.cpython-36.pyc'])]\n"
     ]
    }
   ],
   "source": [
    "print(list(os.walk('/datasets/home/05/205/jmattern/285_proj')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datasets/home/05/205/jmattern/data'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datasets/home/05/205/jmattern/data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
