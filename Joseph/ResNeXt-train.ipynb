{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will go through the process of training the ResNeXt model with an architecture that is optimized for the Cifar Dataset (as specified in [1] https://arxiv.org/pdf/1611.05431.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from srcifar100 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\") # Note, this line is needed only since this notebook is being run from inside 'bharat' directory\n",
    "                      # If you run this notebook in the same folder as the the 'models-py-code' directory, remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set all required parameters\n",
    "Remember to change the `session_id` for each new model you wish to train or if you change of the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "\n",
    "# Input Data Parameters\n",
    "args['data_path'] = '/datasets/ee285s-public/'\n",
    "args['dataset'] = 'cifar100' # The other option is 'cifar10'\n",
    "args['scale_factor'] = 2\n",
    "\n",
    "# Optimization options\n",
    "args['epochs'] = 300\n",
    "args['start_epoch'] = 0 # Change only if necessary. Used for resuming sessions\n",
    "args['batch_size'] = 24\n",
    "args['learning_rate'] = 0.1\n",
    "args['momentum'] = 0.9 # Momentum\n",
    "args['decay'] = 0.0005 # Weight decay (L2 penalty)\n",
    "args['test_bs'] = 64 # Test Batch Size\n",
    "args['schedule'] = [75, 150] # Decrease learning rate at these epochs\n",
    "args['gamma'] = 0.1 # LR is multiplied by gamma on schedule.\n",
    "\n",
    "# Checkpoints and Session Parameters\n",
    "args['save_dir'] = './' # Folder to save checkpoint file and best model file\n",
    "args['load'] = 'resnext-dbpn-x{}-0-checkpoint.pth'.format(args['scale_factor']) # Path to load Checkpoint file \n",
    "args['session_id'] = '0' # Remember to change the session id for each new model you train\n",
    "\n",
    "# Architecture\n",
    "args['depth'] = 29 # Model depth\n",
    "args['cardinality'] = 8 # Model cardinality (group)\n",
    "args['base_width'] = 64 # Number of channels in each group\n",
    "args['widen_factor'] = 4 # Widen factor. 4 -> 64, 8 -> 128, ...\n",
    "\n",
    "# Acceleration\n",
    "args['ngpu'] = 1 # 0 = CPU\n",
    "args['prefetch'] = 2 # Pre-fetching threads\n",
    "\n",
    "# i/o\n",
    "args['log_dir'] = './log/' # Log folder\n",
    "\n",
    "# Create a Dictionary to describe the state every epoch\n",
    "epoch_state = {'args' : args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exisiting log resnext-dbpn-x2-0-log.txt loaded.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args['log_dir']):\n",
    "        os.makedirs(args['log_dir'])\n",
    "\n",
    "log_file_name = 'resnext-dbpn-x{}-{}-log.txt'.format(args['scale_factor'],args['session_id'])\n",
    "if args['load'] == '':\n",
    "    log = open(os.path.join(args['log_dir'], log_file_name), 'w')\n",
    "else:\n",
    "    log = open(os.path.join(args['log_dir'], log_file_name), 'a')    \n",
    "print('Exisiting log {} loaded.'.format(log_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir, '285_proj'))\n",
    "\n",
    "if args['scale_factor'] == 2:\n",
    "    train_folder = 'X2CIFAR100'\n",
    "    test_folder = 'X2CIFAR100'\n",
    "    crop_size = 64\n",
    "elif args['scale_factor'] == 4:\n",
    "    train_folder = 'X4CIFAR100'\n",
    "    test_folder = 'X4CIFAR100'\n",
    "    crop_size = 128\n",
    "    \n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomCrop(crop_size, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "# if args['dataset'] == 'cifar10':\n",
    "#     train_data = dset.CIFAR10(args['data_path'], train=True, transform=train_transform, download=False)\n",
    "#     test_data = dset.CIFAR10(args['data_path'], train=False, transform=test_transform, download=False)\n",
    "#     nlabels = 10\n",
    "# else:\n",
    "#     train_data = dset.CIFAR100(args['data_path'], train=True, transform=train_transform, download=False)\n",
    "#     test_data = dset.CIFAR100(args['data_path'], train=False, transform=test_transform, download=False)\n",
    "nlabels = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(SRCIFAR100(os.path.join(data_dir,train_folder), train_transform), \n",
    "                                           batch_size=args['batch_size'], shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(SRCIFAR100(os.path.join(data_dir,test_folder), test_transform), \n",
    "                                         batch_size=args['batch_size'], shuffle=True, pin_memory=True)\n",
    "    \n",
    "    \n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=args['batch_size'], shuffle=True,\n",
    "#                                                                        num_workers=args['prefetch'], pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=args['test_bs'], shuffle=False,\n",
    "#                                                                      num_workers=args['prefetch'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model, Criterion, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../pymodels/resnext_dbpn_x2.py:97: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.classifier.weight)\n",
      "../pymodels/resnext_dbpn_x2.py:102: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CifarResNeXtX2(\n",
      "  (conv_1_3x3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (stage_1): Sequential(\n",
      "    (stage_1_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_1_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (stage_1_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (stage_2): Sequential(\n",
      "    (stage_2_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_2_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (stage_2_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (stage_3): Sequential(\n",
      "    (stage_3_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_3_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (stage_3_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=82944, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if args['scale_factor'] == 2:\n",
    "    from pymodels.resnext_dbpn_x2 import CifarResNeXtX2 as CifarResNeXtSR # This is where we actually refer to the model we wish to train\n",
    "elif args['scale_factor'] == 4:\n",
    "    from pymodels.resnext_dbpn_x4 import CifarResNeXtX4 as CifarResNeXtSR # This is where we actually refer to the model we wish to train\n",
    "\n",
    "resnext_model = CifarResNeXtSR(args['cardinality'], args['depth'], nlabels, args['base_width'], args['widen_factor'])\n",
    "print(resnext_model)\n",
    "if args['ngpu'] > 1:\n",
    "    resnext_model = torch.nn.DataParallel(resnext_model, device_ids=list(range(args['ngpu'])))\n",
    "\n",
    "if args['ngpu'] > 0:\n",
    "    resnext_model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(resnext_model.parameters(), args['learning_rate'], momentum=args['momentum'],\n",
    "                            weight_decay=args['decay'], nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    resnext_model.train()\n",
    "    loss_avg = 0.0\n",
    "    t0 = time.time()\n",
    "    i = 0\n",
    "    t0 = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n",
    "        \n",
    "        # forward\n",
    "        output = resnext_model(data)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.2 + float(loss) * 0.8\n",
    "        i += 1\n",
    "        if not i%30:\n",
    "            i=0\n",
    "            t1 = time.time()\n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.4f} || Timer: {:.4f} sec.\".format(epoch, batch_idx, len(train_loader), loss.data[0], (t1 - t0)))\n",
    "            t0 = time.time()\n",
    "    epoch_state['train_loss'] = loss_avg\n",
    "    t1 = time.time()\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f} | Time: {:.4f}\".format(epoch, loss_avg, (t1-t0)))\n",
    "\n",
    "# test function (forward only)\n",
    "def test():\n",
    "    resnext_model.eval()\n",
    "    loss_avg = 0.0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n",
    "\n",
    "        # forward\n",
    "        output = resnext_model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # accuracy\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += float(pred.eq(target.data).sum())\n",
    "\n",
    "        # test loss average\n",
    "        loss_avg += float(loss)\n",
    "\n",
    "    epoch_state['test_loss'] = loss_avg / len(test_loader)\n",
    "    epoch_state['test_accuracy'] = correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to save a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='resnext-dbpn-x{}-{}-checkpoint.pth'.format(args['scale_factor'],args['session_id'])):\n",
    "    filepath = os.path.join(args['save_dir'], filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        bestfilepath = os.path.join(args['save_dir'], 'model-best-resnext-dbpn-x{}-{}.pth'.format(args['scale_factor'],args['session_id']))\n",
    "        shutil.copyfile(filepath, bestfilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Resume from checkpoint (if set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> No checkpoint found at 'resnext-dbpn-x2-0-checkpoint.pth'\n"
     ]
    }
   ],
   "source": [
    "if not args['load'] == '':\n",
    "    if os.path.isfile(args['load']):\n",
    "        print(\"=> Loading Checkpoint '{}'\".format(args['load']))\n",
    "        checkpoint = torch.load(args['load'])\n",
    "        args['start_epoch'] = checkpoint['epoch'] + 1\n",
    "        args['learning_rate'] = checkpoint['learning_rate']\n",
    "        resnext_model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> Loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args['load'], checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> No checkpoint found at '{}'\".format(args['load']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch[0](29/2084): Loss: 5.6608 || Timer: 19.2533 sec.\n",
      "===> Epoch[0](59/2084): Loss: 4.5970 || Timer: 18.3048 sec.\n",
      "===> Epoch[0](89/2084): Loss: 4.6217 || Timer: 17.4924 sec.\n",
      "===> Epoch[0](119/2084): Loss: 4.6204 || Timer: 18.1927 sec.\n",
      "===> Epoch[0](149/2084): Loss: 4.5536 || Timer: 16.6145 sec.\n",
      "===> Epoch[0](179/2084): Loss: 4.6027 || Timer: 18.5922 sec.\n",
      "===> Epoch[0](209/2084): Loss: 4.5804 || Timer: 17.7031 sec.\n",
      "===> Epoch[0](239/2084): Loss: 4.6484 || Timer: 17.2047 sec.\n",
      "===> Epoch[0](269/2084): Loss: 4.6157 || Timer: 17.4900 sec.\n",
      "===> Epoch[0](299/2084): Loss: 4.6214 || Timer: 17.6921 sec.\n",
      "===> Epoch[0](329/2084): Loss: 4.6119 || Timer: 17.8156 sec.\n",
      "===> Epoch[0](359/2084): Loss: 4.6429 || Timer: 18.3922 sec.\n",
      "===> Epoch[0](389/2084): Loss: 4.6252 || Timer: 17.5045 sec.\n",
      "===> Epoch[0](419/2084): Loss: 4.6377 || Timer: 19.5898 sec.\n",
      "===> Epoch[0](449/2084): Loss: 4.6432 || Timer: 18.6098 sec.\n",
      "===> Epoch[0](479/2084): Loss: 4.6256 || Timer: 19.0433 sec.\n",
      "===> Epoch[0](509/2084): Loss: 4.5827 || Timer: 18.3497 sec.\n",
      "===> Epoch[0](539/2084): Loss: 4.6458 || Timer: 19.0031 sec.\n",
      "===> Epoch[0](569/2084): Loss: 4.6215 || Timer: 18.9972 sec.\n",
      "===> Epoch[0](599/2084): Loss: 4.6498 || Timer: 18.3033 sec.\n",
      "===> Epoch[0](629/2084): Loss: 4.6068 || Timer: 17.9893 sec.\n",
      "===> Epoch[0](659/2084): Loss: 4.6342 || Timer: 18.5043 sec.\n",
      "===> Epoch[0](689/2084): Loss: 4.6110 || Timer: 18.1018 sec.\n",
      "===> Epoch[0](719/2084): Loss: 4.6198 || Timer: 17.6086 sec.\n",
      "===> Epoch[0](749/2084): Loss: 4.5831 || Timer: 17.5893 sec.\n",
      "===> Epoch[0](779/2084): Loss: 4.5980 || Timer: 18.1395 sec.\n",
      "===> Epoch[0](809/2084): Loss: 4.5282 || Timer: 19.1619 sec.\n",
      "===> Epoch[0](839/2084): Loss: 4.6402 || Timer: 18.5905 sec.\n",
      "===> Epoch[0](869/2084): Loss: 4.6238 || Timer: 18.0164 sec.\n",
      "===> Epoch[0](899/2084): Loss: 4.6415 || Timer: 18.2043 sec.\n",
      "===> Epoch[0](929/2084): Loss: 4.6731 || Timer: 17.6803 sec.\n",
      "===> Epoch[0](959/2084): Loss: 4.5857 || Timer: 18.8069 sec.\n",
      "===> Epoch[0](989/2084): Loss: 4.6789 || Timer: 18.0895 sec.\n",
      "===> Epoch[0](1019/2084): Loss: 4.6398 || Timer: 17.7106 sec.\n",
      "===> Epoch[0](1049/2084): Loss: 4.6401 || Timer: 18.0078 sec.\n",
      "===> Epoch[0](1079/2084): Loss: 4.5907 || Timer: 18.8811 sec.\n",
      "===> Epoch[0](1109/2084): Loss: 4.6133 || Timer: 18.3679 sec.\n",
      "===> Epoch[0](1139/2084): Loss: 4.5808 || Timer: 18.0513 sec.\n",
      "===> Epoch[0](1169/2084): Loss: 4.6312 || Timer: 18.3964 sec.\n",
      "===> Epoch[0](1199/2084): Loss: 4.5953 || Timer: 16.5968 sec.\n",
      "===> Epoch[0](1229/2084): Loss: 4.6410 || Timer: 17.0318 sec.\n",
      "===> Epoch[0](1259/2084): Loss: 4.6233 || Timer: 19.0551 sec.\n",
      "===> Epoch[0](1289/2084): Loss: 4.6202 || Timer: 17.8092 sec.\n",
      "===> Epoch[0](1319/2084): Loss: 4.6410 || Timer: 17.4968 sec.\n",
      "===> Epoch[0](1349/2084): Loss: 4.5915 || Timer: 18.3018 sec.\n",
      "===> Epoch[0](1379/2084): Loss: 4.5760 || Timer: 18.9966 sec.\n",
      "===> Epoch[0](1409/2084): Loss: 4.5724 || Timer: 17.9372 sec.\n",
      "===> Epoch[0](1439/2084): Loss: 4.6001 || Timer: 18.1742 sec.\n",
      "===> Epoch[0](1469/2084): Loss: 4.6936 || Timer: 17.3880 sec.\n",
      "===> Epoch[0](1499/2084): Loss: 4.6216 || Timer: 17.8012 sec.\n",
      "===> Epoch[0](1529/2084): Loss: 4.5929 || Timer: 19.5017 sec.\n",
      "===> Epoch[0](1559/2084): Loss: 4.5922 || Timer: 18.1968 sec.\n",
      "===> Epoch[0](1589/2084): Loss: 4.6511 || Timer: 17.5075 sec.\n",
      "===> Epoch[0](1619/2084): Loss: 4.6432 || Timer: 17.1893 sec.\n",
      "===> Epoch[0](1649/2084): Loss: 4.6209 || Timer: 18.6946 sec.\n",
      "===> Epoch[0](1679/2084): Loss: 4.6811 || Timer: 18.3529 sec.\n",
      "===> Epoch[0](1709/2084): Loss: 4.6318 || Timer: 17.6519 sec.\n",
      "===> Epoch[0](1739/2084): Loss: 4.6372 || Timer: 18.2015 sec.\n",
      "===> Epoch[0](1769/2084): Loss: 4.5584 || Timer: 19.3028 sec.\n",
      "===> Epoch[0](1799/2084): Loss: 4.6265 || Timer: 18.0007 sec.\n",
      "===> Epoch[0](1829/2084): Loss: 4.5882 || Timer: 18.1952 sec.\n",
      "===> Epoch[0](1859/2084): Loss: 4.5990 || Timer: 18.3966 sec.\n",
      "===> Epoch[0](1889/2084): Loss: 4.5971 || Timer: 19.2014 sec.\n",
      "===> Epoch[0](1919/2084): Loss: 4.5983 || Timer: 17.6002 sec.\n",
      "===> Epoch[0](1949/2084): Loss: 4.6143 || Timer: 17.3108 sec.\n",
      "===> Epoch[0](1979/2084): Loss: 4.5227 || Timer: 18.8901 sec.\n",
      "===> Epoch[0](2009/2084): Loss: 4.5963 || Timer: 18.0067 sec.\n",
      "===> Epoch[0](2039/2084): Loss: 4.6190 || Timer: 18.3987 sec.\n",
      "===> Epoch[0](2069/2084): Loss: 4.6569 || Timer: 17.9966 sec.\n",
      "===> Epoch 0 Complete: Avg. Loss: 4.6387 | Time: 8.6949\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f07035ddfe0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b1fd9af005b3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece285_s18_project/pymodels/resnext_dbpn_x2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1_3x3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece285_s18_project/pymodels/resnext_dbpn_x2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_reduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_expand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_expand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.batch_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(args['start_epoch'], args['epochs']):\n",
    "    if epoch in args['schedule']:\n",
    "        args['learning_rate'] *= args['gamma']\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args['learning_rate']\n",
    "\n",
    "    epoch_state['epoch'] = epoch\n",
    "    train(epoch)\n",
    "    test()\n",
    "    if epoch_state['test_accuracy'] > best_accuracy:\n",
    "        best_accuracy = epoch_state['test_accuracy']\n",
    "        isbest = True\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch' : epoch,\n",
    "        'learning_rate' : args['learning_rate'],\n",
    "        'state_dict' : resnext_model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()\n",
    "    }, isbest)\n",
    "    \n",
    "    log.write('%s\\n' % json.dumps(epoch_state))\n",
    "    log.flush()\n",
    "    #print(epoch_state)\n",
    "    print(\"[Epoch {}] Best accuracy: {:.4f}\".format(epoch, best_accuracy))\n",
    "\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
