{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will go through the process of training the ResNeXt model with an architecture that is optimized for the Cifar Dataset (as specified in [1] https://arxiv.org/pdf/1611.05431.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\") # Note, this line is needed only since this notebook is being run from inside 'bharat' directory\n",
    "                      # If you run this notebook in the same folder as the the 'models-py-code' directory, remove this line\n",
    "from pymodels.resnext import CifarResNeXt # This is where we actually refer to the model we wish to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set all required parameters\n",
    "Remember to change the `session_id` for each new model you wish to train or if you change of the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "\n",
    "# Input Data Parameters\n",
    "args['data_path'] = '/datasets/ee285s-public/'\n",
    "args['dataset'] = 'cifar100' # The other option is 'cifar10'\n",
    "\n",
    "# Optimization options\n",
    "args['epochs'] = 150\n",
    "args['start_epoch'] = 0 # Change only if necessary. Used for resuming sessions\n",
    "args['batch_size'] = 96\n",
    "args['learning_rate'] = 0.1\n",
    "args['momentum'] = 0.9 # Momentum\n",
    "args['decay'] = 0.0005 # Weight decay (L2 penalty)\n",
    "args['test_bs'] = 64 # Test Batch Size\n",
    "args['schedule'] = [75, 115] # Decrease learning rate at these epochs\n",
    "args['gamma'] = 0.1 # LR is multiplied by gamma on schedule.\n",
    "\n",
    "# Checkpoints and Session Parameters\n",
    "args['save_dir'] = './' # Folder to save checkpoint file and best model file\n",
    "args['load'] = 'resnext-0-checkpoint.pth' # Path to load Checkpoint file \n",
    "args['session_id'] = '1' # Remember to change the session id for each new model you train\n",
    "\n",
    "# Architecture\n",
    "args['depth'] = 29 # Model depth\n",
    "args['cardinality'] = 8 # Model cardinality (group)\n",
    "args['base_width'] = 64 # Number of channels in each group\n",
    "args['widen_factor'] = 4 # Widen factor. 4 -> 64, 8 -> 128, ...\n",
    "\n",
    "# Acceleration\n",
    "args['ngpu'] = 1 # 0 = CPU\n",
    "args['prefetch'] = 2 # Pre-fetching threads\n",
    "\n",
    "# i/o\n",
    "args['log_dir'] = './log/' # Log folder\n",
    "\n",
    "# Create a Dictionary to describe the state every epoch\n",
    "epoch_state = {'args' : args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exisiting log resnext-0-log.txt loaded.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args['log_dir']):\n",
    "        os.makedirs(args['log_dir'])\n",
    "\n",
    "log_file_name = 'resnext-{}-log.txt'.format(args['session_id'])\n",
    "if args['load'] == '':\n",
    "    log = open(os.path.join(args['log_dir'], log_file_name), 'w')\n",
    "else:\n",
    "    log = open(os.path.join(args['log_dir'], log_file_name), 'a')    \n",
    "print('Exisiting log {} loaded.'.format(log_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "if args['dataset'] == 'cifar10':\n",
    "    train_data = dset.CIFAR10(args['data_path'], train=True, transform=train_transform, download=False)\n",
    "    test_data = dset.CIFAR10(args['data_path'], train=False, transform=test_transform, download=False)\n",
    "    nlabels = 10\n",
    "else:\n",
    "    train_data = dset.CIFAR100(args['data_path'], train=True, transform=train_transform, download=False)\n",
    "    test_data = dset.CIFAR100(args['data_path'], train=False, transform=test_transform, download=False)\n",
    "    nlabels = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=args['batch_size'], shuffle=True,\n",
    "                                                                       num_workers=args['prefetch'], pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=args['test_bs'], shuffle=False,\n",
    "                                                                     num_workers=args['prefetch'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model, Criterion, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CifarResNeXt(\n",
      "  (conv_1_3x3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (stage_1): Sequential(\n",
      "    (stage_1_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_1_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "    (stage_1_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage_2): Sequential(\n",
      "    (stage_2_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_2_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "    (stage_2_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage_3): Sequential(\n",
      "    (stage_3_bottleneck_0): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "        (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (stage_3_bottleneck_1): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "    (stage_3_bottleneck_2): ResNeXtBottleneck(\n",
      "      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (shortcut): Sequential(\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnext_model = CifarResNeXt(args['cardinality'], args['depth'], nlabels, args['base_width'], args['widen_factor'])\n",
    "print(resnext_model)\n",
    "if args['ngpu'] > 1:\n",
    "    resnext_model = torch.nn.DataParallel(resnext_model, device_ids=list(range(args['ngpu'])))\n",
    "\n",
    "if args['ngpu'] > 0:\n",
    "    resnext_model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(resnext_model.parameters(), args['learning_rate'], momentum=args['momentum'],\n",
    "                            weight_decay=args['decay'], nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    resnext_model.train()\n",
    "    loss_avg = 0.0\n",
    "    t0 = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n",
    "\n",
    "        # forward\n",
    "        output = resnext_model(data)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        t0 = time.time()\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        t1 = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.2 + float(loss) * 0.8\n",
    "\n",
    "        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f} || Timer: {:.4f} sec.\".format(epoch, batch_idx, len(train_loader), loss.data[0], (t1 - t0)))\n",
    "\n",
    "    epoch_state['train_loss'] = loss_avg\n",
    "    t1 = time.time()\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f} | Time: {:.4f}\".format(epoch, loss_avg, (t1-t0)))\n",
    "\n",
    "# test function (forward only)\n",
    "def test():\n",
    "    resnext_model.eval()\n",
    "    loss_avg = 0.0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n",
    "\n",
    "        # forward\n",
    "        output = resnext_model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # accuracy\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += float(pred.eq(target.data).sum())\n",
    "\n",
    "        # test loss average\n",
    "        loss_avg += float(loss)\n",
    "\n",
    "    epoch_state['test_loss'] = loss_avg / len(test_loader)\n",
    "    epoch_state['test_accuracy'] = correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to save a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='resnext-{}-checkpoint.pth'.format(args['session_id'])):\n",
    "    filepath = os.path.join(args['save_dir'], filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        bestfilepath = os.path.join(args['save_dir'], 'model-best-resnext-{}.pth'.format(args['session_id']))\n",
    "        shutil.copyfile(filepath, bestfilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Resume from checkpoint (if set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args['load'] == '':\n",
    "    if os.path.isfile(args['load']):\n",
    "        print(\"=> Loading Checkpoint '{}'\".format(args['load']))\n",
    "        checkpoint = torch.load(args['load'])\n",
    "        args['start_epoch'] = checkpoint['epoch'] + 1\n",
    "        args['learning_rate'] = checkpoint['learning_rate']\n",
    "        resnext_model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> Loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args['load'], checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> No checkpoint found at '{}'\".format(args['load']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(args['start_epoch'], args['epochs']):\n",
    "    if epoch in args['schedule']:\n",
    "        args['learning_rate'] *= args['gamma']\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args['learning_rate']\n",
    "\n",
    "    epoch_state['epoch'] = epoch\n",
    "    train(epoch)\n",
    "    test()\n",
    "    if epoch_state['test_accuracy'] > best_accuracy:\n",
    "        best_accuracy = epoch_state['test_accuracy']\n",
    "        isbest = True\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch' : epoch,\n",
    "        'learning_rate' : args['learning_rate'],\n",
    "        'state_dict' : resnext_model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()\n",
    "    }, isbest)\n",
    "    \n",
    "    log.write('%s\\n' % json.dumps(epoch_state))\n",
    "    log.flush()\n",
    "    #print(epoch_state)\n",
    "    print(\"[Epoch {}] Best accuracy: {:.4f}\".format(epoch, best_accuracy))\n",
    "\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
